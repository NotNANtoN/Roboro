defaults:
  - env:
  - policy: q
  - hydra/launcher: basic  # joblib (parallel) or basic (serial)

path:
seed:
render: 0
env_steps: 10000

trainer:
#  weights_summary: "full"
  terminate_on_nan: 0  # Needs to be 0 - otherwise this takes up a third of training time!
  precision: 32  # 16 if args.gpus else 32 - atm precision 16 makes the code slower, not faster

opt:
  name: adam
  eps: 0.00001
  lr: 0.00003

learner:
  train_env:
  # env wrappers params
  sticky_action_prob: 0.0
  frame_stack: 0
  frameskip: 0
  grayscale: 0
  # training params
  steps_per_batch: 1
  batch_size: 32
  num_workers: 0
  # buffer params
  warm_start_size: 1000
  buffer_size: 100000
  n_step: 0
  cer: 0
  per: 0


agent:
  # policy selection
  double_q: 0
  qv: 0
  qvmax: 0
  iqn: 0
  soft_q: 0
  munch_q: 0
  int_ens: 0
  rem: 0
  # learn alg params
  eps_start: 0.1
  # feature extraction net params
  feat_layer_width: 256
  # target net params
  target_net_use_polyak: 1
  target_net_hard_steps: 2000
  target_net_polyak_val: 0.99
  policy:
    gamma: 0.99
    net:
      dueling: 0
      noisy_layers: 0
      width: 256
  
  
# To read the modified CLI args and set the MLflow run_name according to it:
override_args: ${hydra:job.override_dirname}

# Optionally exclude some keys from the run_name:
hydra:
  job:
    config:
      override_dirname:
        exclude_keys:
          - seed
