defaults:
  - env:
  - policy: q

path:
seed:
env_steps: 10000

trainer:
#  weights_summary: "full"
  terminate_on_nan: 0  # Needs to be 0 - otherwise this takes up a third of training time!
  precision: 32  # 16 if args.gpus else 32 - atm precision 16 makes the code slower, not faster

learner:
  train_env:
  # env wrappers params
  frame_stack: 0
  frameskip: 0
  grayscale: 0
  # training params
  steps_per_batch: 1
  learning_rate: 0.00003
  batch_size: 32
  # size params
  warm_start_size: 1000
  buffer_size: 100000

agent:
  # policy selection
  qv: 0
  qvmax: 0
  iqn: 0
  # learn alg params
  eps_start: 0.1
  # feature extraction net params
  feat_layer_width: 256
  # target net params
  target_net_use_polyak: 1
  target_net_hard_steps: 2000
  target_net_polyak_val: 0.99
  policy:
    gamma: 0.99
    net:
      dueling: 0
      noisy_layers: 0
      width: 256
  
  
# To read the modified CLI args and set the MLflow run_name according to it:
override_args: ${hydra:job.override_dirname}

# Optionally exclude some keys from the run_name:
hydra:
  job:
    config:
      override_dirname:
        exclude_keys:
          - seed
