defaults:
  - _self_
  - policy: best
  - env:
  - hydra/launcher: basic  # joblib (parallel) or basic (serial)

path:
seed:
render: 0
env_steps: 10000

trainer:
  gradient_clip_val: 1.0  # 0 means don't clip
  precision: 32  # 16 if args.gpus else 32 - atm precision 16 makes the code slower, not faster

opt:
  name: adam
  eps: 0.00001
  lr: 0.0003

learner:
  train_env:
  # env wrappers params
  sticky_actions: 0.0
  frame_stack: 0
  frameskip: 0
  grayscale: 0
  discretize_actions: false
  num_bins_per_dim: 5
  # training params
  steps_per_batch: 1
  batch_size: 32
  num_workers: 0
  warm_start_size: 1000

buffer:
  # buffer params
  update_freq: 0
  buffer_size: 100000
  n_step: 0
  cer: 0
  per: 0
  # per args - commented out until we figured out a good way to combine configs in a binary way
  # alpha: 0.6
  # beta_start: 0.4

agent:
  # policy selection
  double_q: 0
  qv: 0
  qvmax: 0
  iqn: 0
  soft_q: 0
  munch_q: 0
  int_ens: 0
  rem: 0
  # New discretization parameters
  discretization_method: "joint"  # Options: "joint", "independent_dims"
  num_bins_per_dim: 5           # Used if discretization_method is "independent_dims", or by wrapper if "joint"
  # learn alg params
  clip_rewards: 0
  eps_start: 1.0
  eps_end: 0.05
  eps_decay_steps: 10000
  # feature extraction net params
  feat_layer_width: 256
  # target net params
  target_net_use_polyak: 1
  target_net_hard_steps: 2000
  target_net_polyak_val: 0.99
  policy:
    gamma: 0.99
    net:
      dueling: 0
      noisy_layers: 0
      width: 256
      activation_fn: "relu"  # Options: "relu", "mish"
      use_layer_norm: false  # Whether to use layer normalization after each layer
      dropout_rate: 0.0  # Dropout rate (None for no dropout)


# To read the modified CLI args and set the wandb run_name according to it:
override_args: ${hydra:job.override_dirname}

# Optionally exclude some keys from the run_name:
hydra:
  job:
    config:
      override_dirname:
        exclude_keys:
          - seed
